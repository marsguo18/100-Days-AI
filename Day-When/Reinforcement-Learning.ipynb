{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "**Reinforcement learning (RL)** is the machine learning task of taking suitable action to maximize reward in a particular situation.\n",
    "\n",
    "## Concepts of RL\n",
    "- Agent\n",
    "- Action\n",
    "- Environment\n",
    "- State\n",
    "- Reward\n",
    "- policy\n",
    "- value\n",
    "- Q-value or action-value\n",
    "\n",
    "### 1. Agent\n",
    "An **agent** takes actions; for example, a drone making a delivery, or Super Mario moving in a video game.\n",
    "\n",
    "### 2. Action (A)\n",
    "**A** is the set of all possible moves the **agent** can make. It should be noted that the agent is choosing from a list of possible actions. For example, when we handling a drone, the list might include many different velocities and accelerations in 3D space. And in video games, the list might include running or jumping.\n",
    "\n",
    "### 3. Environment\n",
    "**Environment** is the world where **agent** moves. The environment takes the agent’s current **state** and **action** as input, and returns as output the agent’s **reward** and its **next state**.\n",
    "\n",
    "### 4. State (S)\n",
    "A **state** is the specific instant condition in which the agent is located.\n",
    "\n",
    "### 5. Reward (R)\n",
    "A **reward** is the feedback on the success or failure of an **agent's actions**. For example, in a video game, Mario wins scores when he touches a coin. Rewards may be immediate or delayed, but they can effectively assess the actions of the agent.\n",
    "\n",
    "### 6. Policy (π)\n",
    "**Policy** is the strategy that the agent uses to make the next **action** based on the **current state**.\n",
    "\n",
    "### 7. Value (V)\n",
    "The expected **long-term gains (Value)** with discount, **instead of** the short-term **reward R**. Vπ(s) is defined as the expected long-term return based on the **policy π** when the **current state** is **s**.<br>\n",
    "π + s -> Vπ(s)\n",
    "\n",
    "### 8. Q-value or action-value (Q)\n",
    "**Q-Value** is similar to the above, except that it uses another parameter, the **current action a**. The Vπ(s) here is defined as the expected long-term return based on the **policy π**, **action a** and **current state (s)**.<br>\n",
    "π + a + s -> Vπ(s)\n",
    "\n",
    "## Relationships between Concepts\n",
    "![relationships between concepts](../Images/RL_concepts_relationship.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
