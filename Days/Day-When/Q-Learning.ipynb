{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "**Q-learning**is a values-based learning algorithm in reinforcement learning. The goal of Q-learning is to learn a **policy**, which tells an **agent** what **action** to take in its **current state**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning equation\n",
    "$ Q(s,a) = R(s,a) + \\gamma max{Q(s',a')} $<br>\n",
    "- $Q(s,a)$ : <br>\n",
    "s = current state<br>\n",
    "a = immediate action\n",
    "\n",
    "- $R(s,a)$ : <br>\n",
    "the immediate reward\n",
    "\n",
    "- $\\gamma$ : <br>\n",
    "discount factor\n",
    "\n",
    "- $Q(s',a')$ : <br>\n",
    "s' = next state<br>\n",
    "a' = next action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Rooms example\n",
    "Let's start with an example (example and images of it found from [Mnemosyne_Studio](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)):<br>\n",
    "Suppose we have 5 rooms connected by doors as shown below. There are several things we should care about:\n",
    "1. Rooms are numbered from 0 to 4. \n",
    "2. Our goal is going outside (go to the 5)\n",
    "3. If we want to go into a room, we must go through the door. For example, room 2 to room 1 is **not available**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5rooms](../../Images/5rooms.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better understanding, we can represent the rooms on a graph, each room as a node, and each door as a link. (Colorful link will be mentioned later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph of 5-rooms](../../Images/simplify-5rooms.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to inspire the agent to do a better job, we should add rewards. As we know, 5 is our goal state. So the doors that lead immediately to the goal have an reward of 100. Other doors not directly connected to the target room have zero reward. Because doors are two-way, each arrow contains different reward value, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5rooms-add-reward](../../Images/5rooms-add-reward.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better understanding, we can represent the graph on a matrix, each room as a node, and each door as a link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../Images/matrix-Q-learning-example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning algorithm process:\n",
    "\n",
    "#### 1. Set the discount factor $ \\gamma $, and all rewards in matrix R.\n",
    "\n",
    "#### 2. Initialize:<br>\n",
    "- Q: =0\n",
    "\n",
    "#### 3. For each episode:\n",
    "- For: loop\n",
    "- episode: From the beginning to the end of the event\n",
    "    \n",
    "##### 3.1 Select a random initial state.\n",
    "##### 3.2 Do While the goal state hasn't been reached.\n",
    "3.2.1 Select **one** (a) among all possible actions (A) for the **current state**.<br>\n",
    "3.2.2 Using this **possible action**, consider going to the **next state**.<br>\n",
    "3.2.3 Calculate $ Q(s,a) $<br>\n",
    "3.2.4 Set the **next state** as the **current state**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
