{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "**Q-learning**is a **values-based** learning algorithm in reinforcement learning. The goal of Q-learning is to learn a **policy**, which tells an **agent** what **action** to take in its **current state**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning equation\n",
    "$ Q(s,a) = R(s,a) + \\gamma max{Q(s',a')} $<br>\n",
    "- $Q(s,a)$ : <br>\n",
    "s = current state<br>\n",
    "a = immediate action\n",
    "\n",
    "- $R(s,a)$ : <br>\n",
    "the immediate reward\n",
    "\n",
    "- $\\gamma$ : <br>\n",
    "discount factor\n",
    "\n",
    "- $Q(s',a')$ : <br>\n",
    "s' = next state<br>\n",
    "a' = next action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Rooms example\n",
    "Let's start with an example (example and images of it found from [Mnemosyne_Studio](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)):<br>\n",
    "Suppose we have 5 rooms connected by doors as shown below. There are several things we should care about:\n",
    "1. Rooms are numbered from 0 to 4. \n",
    "2. Our goal is going outside (go to the 5)\n",
    "3. If we want to go into a room, we must go through the door. For example, room 2 to room 1 is **not available**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5rooms](../Images/5rooms.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better understanding, we can represent the rooms on a graph, each room as a node, and each door as a link. (Colorful link will be mentioned later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph of 5-rooms](../Images/simplify-5rooms.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to inspire the agent to do a better job, we should add rewards. As we know, 5 is our **goal state**. So the doors that lead immediately to the goal have an reward of **100**. Other doors not directly connected to the target room have **zero** reward. Because doors are two-way, each arrow contains different reward value. For example, 1 -> 5 and 1 -> 3 are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5rooms-add-reward](../Images/5rooms-add-reward.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our next step, we then represent the graph on a **reward matrix**, matrix R, where **column** represent **State** and **row** means **Action**. Values in the matrix represent the **immediate reward**. The -1's in the table represent null values (i.e.; where there isn't a link between nodes). For example, State 0 cannot go to State 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../Images/matrix-R.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning algorithm process:\n",
    "\n",
    "### 1. Set the discount factor $ \\gamma $ and all rewards in matrix R.\n",
    "\n",
    "### 2. Initialize:<br>\n",
    "- Q: =0\n",
    "\n",
    "<div>\n",
    "    <img align=left src=\"../Images/matrix-Q.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. For each episode:\n",
    "- For: loop\n",
    "- episode: From the **beginning** to the **end** of the event\n",
    "    \n",
    "#### 3.1 Select a random initial state.\n",
    "#### 3.2 Do While the goal state hasn't been reached.\n",
    "3.2.1 Select **one** (a) among all possible actions (A) for the **current state**.<br>\n",
    "3.2.2 Using this **possible action**, consider going to the **next state**.<br>\n",
    "3.2.3 Calculate $ Q(s,a) $<br>\n",
    "3.2.4 Set the **next state** as the **current state** of Matrix Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example By Hand\n",
    "### 1. Set the discount factor $ \\gamma $ and all rewards in matrix R.\n",
    "(1) $ \\gamma = 0.8 $<br>\n",
    "(2) Matrix R:\n",
    "<div>\n",
    "    <img align=left src=\"../Images/matrix-R.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize:<br>\n",
    "- Q: =0\n",
    "\n",
    "<div>\n",
    "    <img align=left src=\"../Images/matrix-Q.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. For each episode: \n",
    "#### 3.1 Select a random initial state.\n",
    "Initial stat = Room 1\n",
    "#### 3.2 Do While the goal state hasn't been reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Select **one** (a) among all possible actions (A) for the **current state**.\n",
    "As we can see from the Matrix R, when the current state is Room 1, there are two values available, state 3 and state 5. By random selection, we select to go to **state 5** as our action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 Using this **possible action**, consider going to the **next state**.<br>\n",
    "Then the next possible states from the **possible action**, **state 5**, are going to **state 1, 4 and 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.3 Calculate $ Q(s,a) $<br>\n",
    "Now we can calculate $ Q(s,a) $ via the Q-learning equation:<br>\n",
    "$ Q(s,a) = R(s,a) + \\gamma max{Q(s',a')} $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Q(1,5) = R(1,5) + 0.8* max{[Q(5,1),Q(5,4),Q(5,5)]} $<br>\n",
    "As we can see from the Matrix R and Matrix Q:<br>\n",
    "- $ R(1,5) = 100 $\n",
    "- $ Q(5,1) = 0 $\n",
    "- $ Q(5,4) = 0 $\n",
    "- $ Q(5,5) = 0 $\n",
    "\n",
    "<div>\n",
    "    <img align=left src=\"../Images/matrix-R.gif\"/><br>\n",
    "    <img src=\"../Images/matrix-Q.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Q(1,5) = 100 + 0.8* 0 = 100$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.4 Set the **next state** as the **current state** of Matrix Q.\n",
    "<div>\n",
    "    <img align=left src=\"../Images/matrix-Q2.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have done the first episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next episode, we restart from step 3.1. This time, we have **state 3** as our **initial state**.\n",
    "- 3.1 Initial state = **state 3**\n",
    "- 3.2.1 Randomly pick **state 1** as our action from three possible **states 1 and 4**\n",
    "- 3.2.2 Next possible states from **state 1**, are going to state 3 and 5.\n",
    "- 3.2.3 $ Q(3,1) = R(3,1) + 0.8*max{[Q(1,3),Q(1,5)]} = 0 + 0.8*100 = 80 $\n",
    "- 3.2.4 Set the **next state** as the **current state** of Matrix Q.\n",
    "\n",
    "<div>\n",
    "    <img align=left src=\"../Images/matrix-Q3.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our **agent** learns more through further episodes, it will finally reach convergence values in matrix Q like:\n",
    "<div>\n",
    "    <img align=left src=\"../Images/matrix-Q4.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Q can then be normalized:\n",
    "If our **agent** learns more through further episodes, it will finally reach convergence values in matrix Q like:\n",
    "<div>\n",
    "    <img align=left src=\"../Images/matrix-Q5.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can represent the Matrix Q on a graph.\n",
    "<div>\n",
    "    <img align=left src=\"../Images/5rooms-add-reward2.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing the best sequences of states is as simple as following the links with the highest values at each state.<br>\n",
    "For example, from initial State 2, the agent can use the matrix Q as a guide:<br>\n",
    "2 -> 3 -> 1 -> 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
